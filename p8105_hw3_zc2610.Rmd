---
title: "p8105_hw3_zc2610"
author: "Zuoqiao Cui"
date: "2022-10-07"
output: github_document
editor_options: 
  chunk_output_type: console
---
```{r}
library(tidyverse)
library(ggplot2)
library(patchwork)
```

## Problem 1

```{r}
library(p8105.datasets)
data("instacart")
```
1. Count the number of aisle and order the number from large to small

2. Make a plot showing the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered.

```{r}
aisle_df = instacart %>% 
  count(aisle) %>% 
  arrange(desc(n)) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  #order the aisle according to the number of items ordered in each aisle
  ggplot(aes(x = aisle,y = n)) +
  geom_point() +
  labs(
    title = "Number of items ordered in each aisle (>10000)",
     x = "Aisle",
     y = "The number of items",
    caption = "Data from instacart"
  ) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1),plot.title = element_text(hjust = 0.5)) 
  #better show the name of each aisle
aisle_df
```
Answer: 

1.There are `r nrow (instacart %>% count(aisle))` aisles

2.Fresh vegetables are the most items ordered from


Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.
```{r}
instacart %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  # number of times to be ordered
  mutate(rank = min_rank(desc(n))) %>% 
  # order by the number of times to be ordered
  filter(rank < 4) %>% 
  arrange(desc(n)) %>% 
  knitr::kable()
# find out the top 3 popular product and the number of being ordered
```

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers
```{r}
instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name,order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_hour) %>% 
#change the data frame from longer to wider
  knitr::kable(digits = 1)
```


## Problem 2
Import data
```{r}
acc_df = read.csv("./data/accel_data.csv") %>% 
  janitor::clean_names()
```
Combine all activity columns
```{r}
acc_df = acc_df %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_minute",
    values_to = "counts",
    names_prefix = "activity_",
  ) %>% 
  mutate(
    activity_minute = as.integer(activity_minute)
  )
```
Create a weekday vs weekend variable
```{r}
acc_df = acc_df %>% 
  mutate(
    weekday_or_weekend = case_when(
      day == "Monday" ~ "weekday",
      day == "Tuesday" ~ "weekday",
      day == "Wednesday" ~ "weekday",
      day == "Thursday" ~ "weekday",
      day == "Friday" ~ "weekday",
      day == "Saturday" ~ "weekend",
      day == "Sunday" ~ "weekend",
      TRUE     ~ ""
    )
  ) 
```

```{r}
colnames(acc_df)
nrow(acc_df)
ncol(acc_df)
```

Description

1. The resulting dataset contains following variables: `r colnames(acc_df)`

2. There are totally `r nrow(acc_df)` observations and `r ncol(acc_df)` variables in the resulting dataset

create a variable that sums up counts of activities in each day and show in a table
```{r}
acc_df %>% 
  group_by(week,day_id,day) %>% 
  summarise(
    total_activity = sum(`counts`)
  ) %>% 
  arrange(desc(total_activity)) %>% 
  knitr::kable(digits = 1)
```
Answer:

1. For week 1, the highest total activity counts appear on Sunday. For week 2,the highest total activity counts appear on Saturday. For week 3,the highest total activity counts appear on Monday.For week 4,the highest total activity counts appear on Wednesday. For week 5,the highest total activity counts appear on Friday.Therefore, from the perspective of the highest total activity counts, there is no apparent trend to follow.
2. The order of total activity counts for the 1st week is Friday > Thursday > Wednesday > Monday > Tuesday > Sunday > Saturday. The order of total activity counts for the 2nd week is Wednesday > Monday > Thursday > Tuesday > Sunday > Friday > Saturday. The order of total activity counts for the 3rd week is Monday > Wednesday > Friday > Sunday > Saturday > Tuesday > Thursday.The order of total activity counts for the 4th week is Saturday > Friday >  Thursday > Wednesday > Tuesday > Sunday >  Monday.The order of total activity counts for the 5th week is Sunday > Friday > Saturday > Thursday > Wednesday > Tuesday > Monday. There is still no apparent trend for order.
3. There are some abnormal data on week 1 Monday, week 4 and week 5 Monday since the total activity counts are too low compared to other days. 


Single panel plot that shows the 24-hour activity time courses for each day

```{r}
acc_df %>% 
   ggplot(aes(x = activity_minute,y = counts,color = day)) +
   geom_point() +
   labs(
    title = "24-Hour Activity Time Courses For Each Day",
    x = "Time(minute)",
    y = "Activity Counts",
    caption = "Data from accel_data"
  ) +
   theme(plot.title = element_text(hjust = 0.5))
# put the title in the center
acc_df
```
Answer:

1. Tuesday,Wednesday and Friday have lower activity counts compared to other days, which means these three days have more stable activity counts. (especially Wednesday)
2. Compared to other minutes, activity counts are higher at around the 1250th minute, which means everyday at around 8 pm, more activity counts are made.

## Problem 3
Load data
```{r}
library(p8105.datasets)
data("ny_noaa")
```
Tidy data
1.Separate date into year, month, day
```{r}
ny_noaa = ny_noaa %>% 
  janitor::clean_names() %>% 
    separate(date,into = c("year","month","day"),sep = "-")
```
2. To ensure observations for temperature, precipitation, and snowfall are given in reasonable units
since the observation for precipitation uses tenths of mm as its unit, we can change it to mm unit. The unit of temperature also needs to be changed from tenths of degrees to degrees

```{r}
ny_noaa = ny_noaa %>% 
  mutate(
    prcp = prcp/10,
    # change the unit of precipitation from tenths of mm to mm for later calculation
    tmax = as.numeric(tmax)/10,
    tmin = as.numeric(tmin)/10
    # change the unit of minimum and maximum temperature from tenths of degrees C to degrees C for later calculation
  )
ny_noaa
```
3. Find the most commonly observed values for snowfall
```{r}
total_num_of_snowfall = (nrow(ny_noaa))
num_of_na_snowfall = sum(is.na(ny_noaa$snow))
percentage_of_na = num_of_na_snowfall/total_num_of_snowfall
num_of_zero_snowfall = nrow(filter(ny_noaa,snow == 0))
percentage_of_zero = num_of_zero_snowfall/total_num_of_snowfall
```
Answer:

The percentage of NA value in snowfall column is `r sum(is.na(ny_noaa$snow))/(nrow(ny_noaa))` and the percentage of 0 value in snowfall column is `r nrow(filter(ny_noaa,snow == 0))/(nrow(ny_noaa))`.

Since the percentage of 0 value in snowfall column is `r nrow(filter(ny_noaa,snow == 0))/(nrow(ny_noaa))` > 0.5, which means more than half of the value for snowfall is 0. Therefore, the most commonly observed values for snowfall is 0.

Change month number to month name
```{r}
ny_noaa = ny_noaa %>% 
  mutate(
     month = recode(month, "01" = "January",
                  "02" = "February",
                  "03" = "March",
                  "04" = "April",
                  "05" = "May",
                  "06" = "June",
                  "07" = "July",
                  "08" = "Augest",
                  "09" = "September",
                  "10" = "October",
                  "11" = "November", 
                  "12" = "December"
                  )
     )
```

Make a two-panel plot show the average max temperature in January and July

```{r}
ny_noaa %>% 
  group_by(id,year,month) %>% 
  filter(month == "January" | month == "July") %>% 
  summarize(
    mean_tmax = mean(tmax,na.rm = TRUE)
  ) %>% 
  ggplot(aes(x = year,y = mean_tmax,color = month)) +
  geom_point() +
 labs(
    title = "Average Max Temperature Plot (January vs July)",
    x = "Year",
    y = "Average Max Temperature",
    caption = "Data from NY NOAA"
  ) +
   theme(plot.title = element_text(hjust = 0.5)) +
  facet_grid(~month)
```
(i) Make a plot show tmax vs tmin 

```{r}
tmax_tmin_df = ny_noaa %>% 
ggplot(aes(x = tmin, y = tmax)) + 
  geom_hex() +
  labs(
    x = "Min temperature",
    y = "Max temperature", 
    title = "tmax vs tmin for the full dataset",
    caption = "Data from NY NOAA"
    ) +
   theme(plot.title = element_text(hjust = 0.5)) 
tmax_tmin_df
```

(ii) Make a plot show the distribution of snowfall values greater than 0 and less than 100 separated by year

```{r}
snowfall_df = ny_noaa %>% 
  mutate(
    year = factor(year)
  ) %>% 
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = year, y = snow)) + 
  geom_boxplot() +
  labs(
    title = "Snowfall Value Plot",
    x = "Year",
    y = "Snowfall Value",
    caption = "Data from NY NOAA"
  ) +
   theme(plot.title = element_text(hjust = 0.5))
snowfall_df
```

Use patchwork to combine two plots above together

```{r}
tmax_tmin_df+snowfall_df
```














