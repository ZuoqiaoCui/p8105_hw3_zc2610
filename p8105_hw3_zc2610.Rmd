---
title: "p8105_hw3_zc2610"
author: "Zuoqiao Cui"
date: "2022-10-07"
output: github_document
editor_options: 
  chunk_output_type: console
---
```{r}
library(tidyverse)
library(ggplot2)
library(patchwork)
```

## Problem 1

```{r}
library(p8105.datasets)
data("instacart")
```
1. Count the number of aisle and order the number from large to small

2. Make a plot showing the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered.

```{r}
aisle_df = instacart %>% 
  count(aisle) %>% 
  arrange(desc(n)) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  #order the aisle according to the number of items ordered in each aisle
  ggplot(aes(x = aisle,y = n)) +
  geom_point() +
  labs(
    title = "Number of items ordered in each aisle (>10000)",
     x = "Aisle",
     y = "The number of items",
    caption = "Data from instacart"
  ) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1),plot.title = element_text(hjust = 0.5)) 
  #better show the name of each aisle
aisle_df
```
Answer: 

1.There are `r nrow (instacart %>% count(aisle))` aisles

2.Fresh vegetables are the most items ordered from


Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.
```{r}
instacart %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  # number of times to be ordered
  mutate(rank = min_rank(desc(n))) %>% 
  # order by the number of times to be ordered
  filter(rank < 4) %>% 
  arrange(desc(n)) %>% 
  knitr::kable()
# find out the top 3 popular product and the number of being ordered
```

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers
```{r}
instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name,order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_hour) %>% 
#change the data frame from longer to wider
  knitr::kable(digits = 1)
```


## Problem 2
Import data
```{r}
acc_df = read.csv("./data/accel_data.csv") %>% 
  janitor::clean_names()
```
Combine all activity columns
```{r}
acc_df = acc_df %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_minute",
    values_to = "counts",
    names_prefix = "activity_",
  ) %>% 
  mutate(
    activity_minute = as.integer(activity_minute)
  )
```
Create a weekday vs weekend variable
```{r}
acc_df = acc_df %>% 
  mutate(
    weekday_or_weekend = case_when(
      day == "Monday" ~ "weekday",
      day == "Tuesday" ~ "weekday",
      day == "Wednesday" ~ "weekday",
      day == "Thursday" ~ "weekday",
      day == "Friday" ~ "weekday",
      day == "Saturday" ~ "weekend",
      day == "Sunday" ~ "weekend",
      TRUE     ~ ""
    )
  ) 
```

```{r}
colnames(acc_df)
nrow(acc_df)
ncol(acc_df)
```

Description

1. The resulting dataset contains following variables: `r colnames(acc_df)`

2. There are totally `r nrow(acc_df)` observations and `r ncol(acc_df)` variables in the resulting dataset

create a variable that sums up counts of activities in each day and show in a table
```{r}
acc_df %>% 
  group_by(week,day_id,day) %>% 
  summarise(
    total_activity = sum(`counts`)
  ) %>% 
  arrange(week,desc(total_activity)) %>% 
  # order the total activity counts for each week
  knitr::kable(digits = 1)
```
Answer:

1. For week 1, the highest total activity counts appear on Sunday. For week 2,the highest total activity counts appear on Saturday. For week 3,the highest total activity counts appear on Monday.For week 4,the highest total activity counts appear on Wednesday. For week 5,the highest total activity counts appear on Friday.Therefore, from the perspective of the highest total activity counts, there is no apparent trend to follow.
2. The order of total activity counts for the 1st week is Friday > Thursday > Wednesday > Monday > Tuesday > Sunday > Saturday. The order of total activity counts for the 2nd week is Wednesday > Monday > Thursday > Tuesday > Sunday > Friday > Saturday. The order of total activity counts for the 3rd week is Monday > Wednesday > Friday > Sunday > Saturday > Tuesday > Thursday.The order of total activity counts for the 4th week is Saturday > Friday >  Thursday > Wednesday > Tuesday > Sunday >  Monday.The order of total activity counts for the 5th week is Sunday > Friday > Saturday > Thursday > Wednesday > Tuesday > Monday. There is still no apparent trend for order.
3. There are some abnormal data on week 1 Monday, week 4 and week 5 Monday since the total activity counts are too low compared to other days. 


Single panel plot that shows the 24-hour activity time courses for each day

```{r}
acc_df %>% 
   ggplot(aes(x = activity_minute,y = counts,color = day)) +
   geom_point() +
   labs(
    title = "24-Hour Activity Time Courses For Each Day",
    x = "Time(minute)",
    y = "Activity Counts",
    caption = "Data from accel_data"
  ) +
   theme(plot.title = element_text(hjust = 0.5))
# put the title in the center
acc_df
```
Answer:

1. Tuesday,Wednesday and Friday have lower activity counts compared to other days, which means these three days have more stable activity counts. (especially Wednesday)
2. Compared to other minutes, activity counts are higher at around the 1250th minute, which means everyday at around 8 pm, more activity counts are made.

## Problem 3
Load data
```{r}
library(p8105.datasets)
data("ny_noaa")
```
Tidy data
1.Separate date into year, month, day
```{r}
ny_noaa = ny_noaa %>% 
  janitor::clean_names() %>% 
    separate(date,into = c("year","month","day"),sep = "-")
```
2. To ensure observations for temperature, precipitation, and snowfall are given in reasonable units
since the observation for precipitation uses tenths of mm as its unit, we can change it to mm unit. The unit of temperature also needs to be changed from tenths of degrees to degrees

```{r}
ny_noaa = ny_noaa %>% 
  mutate(
    prcp = prcp/10,
    # change the unit of precipitation from tenths of mm to mm for later calculation
    tmax = as.numeric(tmax)/10,
    tmin = as.numeric(tmin)/10
    # change the unit of minimum and maximum temperature from tenths of degrees C to degrees C for later calculation
  )
ny_noaa
```
3. Find the most commonly observed values for snowfall
```{r}
total_num_of_snowfall = (nrow(ny_noaa))
num_of_na_snowfall = sum(is.na(ny_noaa$snow))
percentage_of_na = num_of_na_snowfall/total_num_of_snowfall
num_of_zero_snowfall = nrow(filter(ny_noaa,snow == 0))
percentage_of_zero = num_of_zero_snowfall/total_num_of_snowfall
```
Answer:

The percentage of NA value in snowfall column is `r sum(is.na(ny_noaa$snow))/(nrow(ny_noaa))` and the percentage of 0 value in snowfall column is `r nrow(filter(ny_noaa,snow == 0))/(nrow(ny_noaa))`.

Since the percentage of 0 value in snowfall column is `r nrow(filter(ny_noaa,snow == 0))/(nrow(ny_noaa))` > 0.5, which means more than half of the value for snowfall is 0. Therefore, the most commonly observed values for snowfall is 0.

Change month number to month name
```{r}
ny_noaa = ny_noaa %>% 
  mutate(
     month = recode(month, "01" = "January",
                  "02" = "February",
                  "03" = "March",
                  "04" = "April",
                  "05" = "May",
                  "06" = "June",
                  "07" = "July",
                  "08" = "Augest",
                  "09" = "September",
                  "10" = "October",
                  "11" = "November", 
                  "12" = "December"
                  )
     )
```

Make a two-panel plot show the average max temperature in January and July

```{r}
ny_noaa %>% 
  group_by(id,year,month) %>% 
  filter(month == "January" | month == "July") %>% 
  summarize(
    mean_tmax = mean(tmax,na.rm = TRUE)
  ) %>% 
  ggplot(aes(x = year,y = mean_tmax,color = month)) +
  geom_point() +
 labs(
    title = "Average Max Temperature Plot (January vs July)",
    x = "Year",
    y = "Average Max Temperature",
    caption = "Data from NY NOAA"
  ) +
   theme(plot.title = element_text(hjust = 0.5)) +
  facet_grid(~month) 
# separate this panel into two panel plot by January and July
```
Answer:

1. For January, the average tmax is increasing according to the total trend between year 1980 an 1990. There is an obvious decreasing in around 1994. Then the average tmax decreased a lot between 2003 and 2004. Then tmax increased a lot in 2006. Then it kept decreasing. For July, there are fewer varies compared to January. The average tmax is stable in total. In around 1986, 1992,2000 and 2009, the average tmax has an obvious decreasing and there is an obvious increasing in 2010.
2. For the panel, we can see that both January and July have several outliers. For instance, between 1980 and 1985, there is an obvious outlier of tmax in January and one of the 2005 year's average tmaxs is also an outlier. As for July, between 1985 and 1990, there is also an outlier of tmax.

(i) Make a plot show tmax vs tmin 

```{r}
tmax_tmin_df = ny_noaa %>% 
ggplot(aes(x = tmin, y = tmax)) + 
  geom_hex() +
  labs(
    x = "Min temperature",
    y = "Max temperature", 
    title = "tmax vs tmin for the full dataset",
    caption = "Data from NY NOAA"
    ) +
   theme(plot.title = element_text(hjust = 0.5)) 
tmax_tmin_df
```

(ii) Make a plot show the distribution of snowfall values greater than 0 and less than 100 separated by year

For distribution, we always use density but in this problem, we need to show the distribution for each year. Therefore, I think it is better to choose boxplot so that distributions can be separated by each year and it is also helpful for me to find out the outliers in each year

```{r}
snowfall_df = ny_noaa %>% 
  mutate(
    year = factor(year)
  ) %>% 
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = year, y = snow)) + 
  geom_boxplot() +
  labs(
    title = "Snowfall Value Plot",
    x = "Year",
    y = "Snowfall Value",
    caption = "Data from NY NOAA"
  ) +
   theme(plot.title = element_text(hjust = 0.5))
snowfall_df
```

Use patchwork to combine two plots above together

```{r}
tmax_tmin_df+snowfall_df
```














